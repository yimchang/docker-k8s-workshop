# 쿠버네티스 리소스

`Pod` 리소스를 기초로 더 많은 기능들을 내포하고 있는 리소스들을 알아보도록 하겠습니다.

## ReplicaSet

`ReplicaSet`은 이름에서도 알 수 있듯이 `Pod`를 복제(replicate)해 줍니다.
```bash
cat << EOF | kubectl apply -f -
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mynginx-rs
  labels:
    app: mynginx-rs
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mynginx-rs
  template:
    metadata:
      labels:
        app: mynginx-rs
    spec:
      containers:
      - name: nginx
        image: nginx
EOF
```

- `replicas`: 복제할 `Pod`의 개수를 정의합니다. `ReplicaSet`에서 해당 `Pod` 개수를 유지 시켜 줍니다.
- `selector.matchLabels`: `Service`와 마찬가지로 라벨을 통하여 유지시켜야할 `Pod`를 찾습니다. 예시에서는 app: mygninx-rs 라는 라벨을 가진 `Pod`의 개수를 2로 유지한다는 것을 의미합니다.
- `template`: 유지시켜야할 `Pod`를 정의합니다. 지금까지 배운 `Pod`의 spec과 동일합니다.

```bash
kubectl get pod
# NAME              READY   STATUS      RESTARTS   AGE
# mynginx-rs-jc496  1/1     Running     0          6s
```

기존의 `Pod`와는 조금 다르게 뒤에 랜덤 문자열이 붙은 것을 확인할 수 있습니다. `mynginx-rs-XXXX` 그 이유는 해당 `Pod`를 `ReplicaSet`이 만들었기 때문입니다.

```bash
kubectl get replicaset  # 축약시, rs
# NAME          DESIRED   CURRENT   READY   AGE
# mynginx-rs    1         1         1       1m

```

- DESIRED: 유지시켜야할 `Pod`의 개수를 의미합니다.
- CURRENT: 현재 `Pod`의 개수를 의미합니다.
- READY: 현재 생성된 `Pod` 중 정상 동작하고 있는 (livenessProbe) 개수를 의미합니다.

`Pod`의 개수를 2개로 늘려 보겠습니다.

```bash
kubectl scale rs --replicas 2 mynginx-rs

kubectl get rs
# NAME          DESIRED   CURRENT   READY   AGE
# mynginx-rs    2         2         2       1m

kubectl get pod
# NAME              READY   STATUS      RESTARTS   AGE
# mynginx-rs-jc496  1/1     Running     0          2m
# mynginx-rs-dc20x  1/1     Running     0          9s
```

`Pod`를 강제로 삭제하면 어떻게 될까요?

```bash
kubectl delete pod mynginx-rs-jc496
# pod "mynginx-rs-jc496" deleted

kubectl get pod
# NAME              READY   STATUS      RESTARTS   AGE
# mynginx-rs-dc20x  1/1     Running     0          1m
# mynginx-rs-d8kvl  1/1     Running     0          3s
```

`Pod`를 삭제해도 새로운 `Pod`가 `ReplicaSet`에 의해서 생성되는 것을 확인할 수 있습니다.

이처럼 일정 수의 컨테이너를 지속적으로 유지시켜야 하는 경우, (web server 등) `ReplicaSet` 리소스를 이용하면 편리하게 운영할 수 있습니다.

`ReplicaSet` 자체를 삭제해 봅시다.

```bash
kubectl delete rs mynginx-rs
#replicaset.apps "mynginx-rs" deleted

kubectl get rs

kubectl get pod
```

## Deployment

`Deployment` 리소스는 `ReplicaSet` 리소스와 거의 유사하지만 몇가지 기능이 더 있습니다. 리소스의 이름처럼 배포에 특화된 기능이 있습니다. 변경점이 발생하여 새로운 `Pod`를 생성때 기존 `Pod` history를 전부 기억하고 있어 필요할 때마다 손쉽게 rollback을 할 수 있게 해줍니다.
먼저 `Deployment` 리소스를 생성해 보겠습니다. `Deployment`의 정의서는 `ReplicaSet`과 거의 유사하지만 배포 전략 (`strategy`)을 취할 수 있습니다.

```bash
cat << EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mynginx-deploy
  labels:
    app: nginx
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%  
      maxSurge: 25%
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
EOF
```

#### `strategy.type`
배포 전략 종류를 선택한다. `RollingUpdate`와 `Recreate`이 있습니다. `Recreate` 선택시, 새로운 `Pod`가 생성가 생성될 때 기존 `Pod`가 삭제되고 새로운 `Pod`가 생성됩니다. (기존 `Pod` 생성 전략)
`RollingUpdate` 선택시, 점진적으로 업데이트가 일어납니다. 서비스가 중단되면 안되는 웹 페이지 등에서 활용할 수 있습니다. 이때 아래 설정값에 따라 얼마나 점진적으로 업데이트가 일어날지 설정할 수 있습니다.

#### `strategy.rollingUpdate.maxUnavailable`
최대 중단 `Pod` 개수 (혹은 비율). 예를 들어 총 10개 replica에 maxUnavailable의 비율이 25%면 약 3개의 `Pod`가 RollingUpdate시에 일시 중단될 수 있다는 것을 의미합니다.

#### `strategy.rollingUpdate.maxSurge`
최대 동시 `Pod` 개수 (혹은 비율). 예를 들어 총 10개 replica에 maxSurge의 비율이 25%면 약 3개의 `Pod`가 초과하여 최대 13개까지 생성될 수 있다는 것을 의미합니다.

예시의 설정으로 RollingUpdate시, 최소 7개에서 최대 13개까지의 `Pod`가 점진적으로 업데이트 된다고 볼 수 있습니다.

```bash
kubectl get deployment  # 혹은 deploy

#####################
# 배포 변경
#####################
# 기존 nginx 버전 1.7.9에서 1.9.1로 업데이트
kubectl set image deployment mynginx-deploy nginx=nginx:1.9.1 --record
# 혹은 직접 edit
kubectl edit deployment mynginx-deploy
# 업데이트 진행 상황 확인
watch kubectl get pod
```

```bash
#####################
# 배포 상태 확인
#####################
# 업데이트 상태 확인
kubectl rollout status deployment mynginx-deploy
# Deployment 상태 확인
kubectl describe deployment mynginx-deploy
```

```bash
#####################
# 롤백
#####################
# 1.9.1 버전에서 1.9.21 버전으로 업데이트 (에러 발생)
kubectl set image deployment mynginx-deploy nginx=nginx:1.9.21 --record  # no such image version
# 업데이트 히스토리 확인
kubectl rollout history deployment mynginx-deploy
# 잘못 설정된 1.9.21에서 --> 1.9.1로 롤백
kubectl rollout undo deployment mynginx-deploy
# 배포 히스토리 버전 1로 업데이트
kubectl rollout undo deployment mynginx-deploy --to-revision=1
```

```bash
#####################
# replica 수정
#####################
# replica 개수를 5로 업데이트
kubectl scale deployment mynginx-deploy --replicas=6 --record
# 업데이트 히스토리 확인
kubectl rollout history deployment mynginx-deploy
```

```bash
kubectl delete deploy mynginx-deploy
```

## Job & CronJob

### Job

`Job` 리소스는 `Pod`와는 다르게 항상 실행되고 있는 daemon성 프로세스가 아닌 한번 실행하고 완료가 되는 batch성 프로세스를 처리하는 용도로 만들어졌습니다.
간단한 기계학습 모델을 `Job`으로 실행 시켜 보겠습니다.

- `train.py`: 간단한 기계학습 스크립트
- `Dockerfile`: 기계학습 스크립트를 도커 이미지로 변환
- `job.yaml`: `Job` 실행을 위한 리소스 정의서

[`train.py`](train.py) 파일의 상단을 보시면 파라미터를 받는 부분이 있고 맨끝 부분에서는 모델의 성능을 출력하는 부분이 있는 것을 알 수 있습니다.

```python
# train.py

#####################
# parameters
#####################
epochs = int(sys.argv[1])
activate = sys.argv[2]
dropout = float(sys.argv[3])
print(sys.argv)
#####################

# ...
# ...

score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

```

```Dockerfile
FROM python:3.6.8-stretch

RUN pip install tensorflow==1.5 keras==2.0.8 h5py==2.7.1

COPY train.py .

ENTRYPOINT ["python", "train.py"]
```

```bash
# 도커 이미지 빌드
docker build . -t <USERNAME>myfirstjob

# 도커 이미지 업로드
docker push <USERNAME>myfirstjob
```

```yaml
# job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: myfirstjob
spec:
  template:
    spec:
      containers:
      - name: ml
        image: $USERNAME/myfirstjob
        args: ['5', 'softmax', '0.5']
      restartPolicy: Never
  backoffLimit: 2
```

- `template`: `Pod` 리소스의 spec과 동일합니다. `Job`도 결국은 내부적으로 `Pod`를 통해 실행되기 때문입니다.
- `backoffLimit`: restart 횟수를 지정합니다. 예시에서는 총 2번의 restart 시도 후 최종적으로 실패라고 기록이 됩니다.

```bash
# 이미지를 빌드합니다.
docker build . -t $USERNAME/myfirstjob
# 이미지를 업로드합니다.
docker push $USERNAME/myfirstjob

# vi job.yaml --> change image: $USERNAME 
kubectl apply -f job.yaml
# job.batch/myfirstjob created

kubectl get job
# NAME         COMPLETIONS   DURATION   AGE
# myfirstjob   0/1           9s         9s

kubectl get pod
# NAME                                             READY   STATUS      RESTARTS   AGE
# myfirstjob-l5thh                                 1/1     Running     0          9s

# 로그 확인
kubectl logs -f myfirstjob-l5thh 
# ['train.py', '5', 'softmax', '0.5']
# Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz
# Using TensorFlow backend.
# 11460608/11490434 [============================>.] - ETA: 0s30000 train samples
# 2000 test samples
# _________________________________________________________________
# Layer (type)                 Output Shape              Param #
# =================================================================
# dense_1 (Dense)              (None, 512)               401920
# _________________________________________________________________
# ...

# Job 완료 확인
kubectl get job,pod

kubectl delete job myfirstjob
```

몇가지 짚고 넘어가고 싶은 내용들이 있는데요.
1. 먼저 도커허브에서 레포지토리를 따로 생성하지 않았음에도 불구하고 `docker push` 실행시 정상적으로 업로드가 됩니다. 사실 Web을 통해 명시적으로 저장소를 생성하지 않아도 `docker push`를 통해 저장소가 존재하지 않으면 자동으로 만들어 줍니다. 
2. 내 로컬에 myfirstjob 이미지를 바로 사용하면 되는데 왜 굳이 원격 저장소에 저장하고 그것을 참조해서 `Job`을 실행할까요? 쿠버네티스는 여러 워커 서버들이 연결되어 있는 클러스터 기반의 플랫폼이기 때문에 기본적으로 항상 원격 저장소의 이미지를 사용하게끔 설계가 되어 있습니다.

### CronJob

`CronJob`은 `Job` 리소스와 유사하지만 time-based의 일정 주기로 `Job`을 실행할 수 있도록 확장된 리소스입니다.

```bash
cat << EOF | kubectl apply -f -
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
EOF
```

`CronJob` 생성시 schedule에 등록된 주기마다 등록된 `Job`이 실행되는 것을 확인할 수 있습니다.
```bash
kubectl get cronjob
# NAME    SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
# hello   */1 * * * *   False     0        <none>          4s

kubectl get job
# NAME               COMPLETIONS   DURATION   AGE
# hello-1584873060   0/1           3s         3s

kubectl get pod
# NAME                     READY   STATUS      RESTARTS   AGE
# hello-1593329160-pkkkf   0/1     Completed   0          6s

kubectl logs hello-1593329160-pkkkf
# Sun Jun 28 07:26:12 UTC 2020
# Hello from the Kubernetes cluster
```

## 그 외 리소스

쿠버네티스에는 지금까지 알아본 리소스 외에도 더 많은 리소스가 존재합니다. 심지어 사용자 정의 리소스를 만들어 주는 리소스도 존재합니다. 해당 워크샵에서 모든 리소스를 다루지는 않을 예정입니다.
그 이유로는 리소스의 개수도 많기도 하지만 그러한 이유 보다는 지금까지 알아본 쿠버네티스의 개념만으로도 충분히 새로운 리소스를 파악하는데 무리가 없기 때문입니다.
[쿠버네티스 공식 사이트](https://kubernetes.io/ko/docs/concepts/workloads/controllers/replicaset/)에 가보면 더 많은 리소스를 더 자세히 설명해 주기 때문에 모르는 리소스가 생기는 경우, 직접 확인해 보기 쉽습니다.
아래에 설명 드릴 리소스는 큰 그림에서 개념적으로만 설명 드리겠습니다.

### DaemonSet

모든 노드에 동일한 `Pod`를 실행시키고자 할 때 사용하는 리소스입니다. 리소스 수집기, 로그 수집기 등과 같이 모든 노드에 위치하면서 노드에 관한 정보를 추출할 때 많이 사용합니다.
아래는 fluentd를 이용하여 각 노드의 컨테이너 로그를 elasticsearch로 보내는 `DaemonSet` 예시입니다.

<details>

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
```

</details>

### StatefulSet

Stateful한 `Pod`를 생성해야 하는 경우, `StatefulSet`을 이용합니다. `Deployment`와는 다르게 각 `Pod`가 동일하지 않고 순서 및 고유성을 보장 받습니다. 동일한 스펙을 이용하여 `Pod`를 생성하지만 운영시에는 각기 다른 역할을 가지며 서로 역할을 교체하지 못합니다.
각 노드간(쿠버네티스의 노드가 아닌 일반적인 개념의 노드) 치환될 수 없는 클러스터를 구축할 때 많이 사용합니다.

아래는 카산드라 클러스터를 만들기 위한 `StatefulSet`의 예시입니다.

<details>

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cassandra
  labels:
    app: cassandra
spec:
  serviceName: cassandra
  replicas: 3
  selector:
    matchLabels:
      app: cassandra
  template:
    metadata:
      labels:
        app: cassandra
    spec:
      terminationGracePeriodSeconds: 1800
      containers:
      - name: cassandra
        image: gcr.io/google-samples/cassandra:v13
        imagePullPolicy: Always
        ports:
        - containerPort: 7000
          name: intra-node
        - containerPort: 7001
          name: tls-intra-node
        - containerPort: 7199
          name: jmx
        - containerPort: 9042
          name: cql
        resources:
          limits:
            cpu: "500m"
            memory: 1Gi
          requests:
            cpu: "500m"
            memory: 1Gi
        securityContext:
          capabilities:
            add:
              - IPC_LOCK
        lifecycle:
          preStop:
            exec:
              command: 
              - /bin/sh
              - -c
              - nodetool drain
        env:
          - name: MAX_HEAP_SIZE
            value: 512M
          - name: HEAP_NEWSIZE
            value: 100M
          - name: CASSANDRA_SEEDS
            value: "cassandra-0.cassandra.default.svc.cluster.local"
          - name: CASSANDRA_CLUSTER_NAME
            value: "K8Demo"
          - name: CASSANDRA_DC
            value: "DC1-K8Demo"
          - name: CASSANDRA_RACK
            value: "Rack1-K8Demo"
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - /ready-probe.sh
          initialDelaySeconds: 15
          timeoutSeconds: 5
        volumeMounts:
        - name: cassandra-data
          mountPath: /cassandra_data
  volumeClaimTemplates:
  - metadata:
      name: cassandra-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi
```

</details>

### Secret

앞서 본 `ConfigMap` 리소스와 유사하지만 저장시 base64로 인코딩되어 평문으로 저장되지 않습니다. (단지 인코딩만 될 뿐 암호화는 아닙니다. 암호화 기능을 활성 시킬 수 있으나 실습에서는 생략합니다.)

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  username: YWRtaW4=
  password: MWYyZDFlMmU2N2Rm
```

더 많은 리소스를 확인하고 싶다면 아래 명령을 통해 확인할 수 있습니다.

```bash
kubectl api-resources
```

#### Clean up

```bash
kubectl delete deploy --all
kubectl delete rs --all
kubectl delete pod --all
kubectl delete svc --all
kubectl delete job --all
kubectl delete cronjob --all
kubectl delete ds --all  # 혹은 daemonset
kubectl delete cm --all  # 혹은 configmap
```

---

## :trophy: Do it more

1. [03 네트워킹](https://github.com/hongkunyoo/docker-k8s-workshop/blob/master/k8s/03.md#trophy-do-it-more)에서 만든 어플리케이션을 `Deployment`로 변경해 봅시다.
2. `replica` 개수를 5으로 주어 가용성을 높혀 주시기 바랍니다. (mysql 제외)
3. App에서 사용된 환경변수 및 설정 파일을 전부 `ConfigMap`으로 분리하여 생성해 주시기 바랍니다.
4. 매 1분마다 생성한 어플리케이션을 호출하여 결과를 보여주는 `CronJob`을 생성해 주시기 바랍니다.
